"""
Song Map Position Tracker for Performia.

This module implements the committee-recommended architecture:
- Uses onset detection (not continuous pitch tracking) to sync with performer
- Tracks position through pre-analyzed Song Map
- Handles rubato, section skips, and repeats
- Achieves <25ms latency as validated by benchmarks

The Song Map contains all timing, pitch, and chord information from offline analysis.
This tracker simply determines WHERE the performer is within that map.
"""

import numpy as np
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field
from collections import deque
import time
import bisect


@dataclass
class SongMapSection:
    """A section in the Song Map (verse, chorus, etc.)"""
    name: str  # "Verse 1", "Chorus", etc.
    start_time: float  # Seconds from start of song
    end_time: float  # Seconds from start of song
    lines: List[Dict[str, Any]]  # Lyric lines with syllable timing


@dataclass
class SongMapOnset:
    """A pre-analyzed onset (note attack) in the Song Map"""
    time: float  # Seconds from start of song
    section_index: int  # Which section this onset belongs to
    line_index: int  # Which line within the section
    syllable_index: int  # Which syllable (if applicable)
    pitch: Optional[float] = None  # Hz (if vocal)
    chord: Optional[str] = None  # Chord symbol at this time
    confidence: float = 1.0  # Confidence score from offline analysis


@dataclass
class PerformerPosition:
    """Current position of performer in the Song Map"""
    song_time: float  # Estimated time in the song (seconds)
    section_index: int  # Current section
    line_index: int  # Current line within section
    syllable_index: int  # Current syllable
    confidence: float  # How confident we are (0-1)
    tempo_ratio: float = 1.0  # Performer tempo vs original (1.0 = same speed)
    last_onset_time: Optional[float] = None  # When we last detected an onset


class SongMapPositionTracker:
    """
    Tracks performer position through a pre-analyzed Song Map.

    Key Design Decisions (per committee recommendation):
    1. Use onset detection + Song Map lookup (not real-time pitch/beat tracking)
    2. Song Map contains all timing/pitch/chord data from offline analysis
    3. Handle rubato (tempo variation) by tracking tempo ratio
    4. Handle section skips/repeats by detecting unexpected onset patterns
    5. Target <25ms latency (validated by benchmarks)

    Args:
        song_map: Pre-analyzed Song Map dictionary
        onset_match_window: Time window for matching detected onsets to map (seconds)
        min_onset_confidence: Minimum confidence for using an onset match
        tempo_smoothing: Smoothing factor for tempo ratio updates (0-1)
    """

    def __init__(
        self,
        song_map: Dict[str, Any],
        onset_match_window: float = 0.15,  # 150ms window for matching
        min_onset_confidence: float = 0.6,
        tempo_smoothing: float = 0.3
    ):
        self.song_map = song_map
        self.onset_match_window = onset_match_window
        self.min_onset_confidence = min_onset_confidence
        self.tempo_smoothing = tempo_smoothing

        # Parse Song Map into efficient data structures
        self.sections = self._parse_sections(song_map)
        self.onsets = self._parse_onsets(song_map)
        self.onset_times = [o.time for o in self.onsets]  # For binary search

        # Current position
        self.position = PerformerPosition(
            song_time=0.0,
            section_index=0,
            line_index=0,
            syllable_index=0,
            confidence=0.0
        )

        # Onset tracking
        self.recent_onsets = deque(maxlen=8)  # Last 8 detected onsets
        self.performance_start_time: Optional[float] = None
        self.last_update_time: Optional[float] = None

        # Tempo tracking
        self.tempo_estimates = deque(maxlen=4)  # Rolling tempo ratio estimates

        # Jump detection (for section skips/repeats)
        self.expected_next_onset_idx: Optional[int] = None

    def _parse_sections(self, song_map: Dict[str, Any]) -> List[SongMapSection]:
        """Parse sections from Song Map."""
        sections = []

        for section_data in song_map.get('sections', []):
            # Calculate section timing from first/last syllable
            lines = section_data.get('lines', [])

            if not lines:
                continue

            # Get start time from first syllable
            first_syllable = None
            for line in lines:
                syllables = line.get('syllables', [])
                if syllables:
                    first_syllable = syllables[0]
                    break

            # Get end time from last syllable
            last_syllable = None
            for line in reversed(lines):
                syllables = line.get('syllables', [])
                if syllables:
                    last_syllable = syllables[-1]
                    break

            if first_syllable and last_syllable:
                start_time = first_syllable.get('startTime', 0.0)
                end_time = last_syllable.get('startTime', 0.0) + last_syllable.get('duration', 0.0)

                sections.append(SongMapSection(
                    name=section_data.get('name', 'Unknown'),
                    start_time=start_time,
                    end_time=end_time,
                    lines=lines
                ))

        return sections

    def _parse_onsets(self, song_map: Dict[str, Any]) -> List[SongMapOnset]:
        """Parse onsets from Song Map syllable timing."""
        onsets = []

        for section_idx, section_data in enumerate(song_map.get('sections', [])):
            lines = section_data.get('lines', [])

            for line_idx, line in enumerate(lines):
                syllables = line.get('syllables', [])

                for syllable_idx, syllable in enumerate(syllables):
                    # Each syllable start is an onset
                    onset_time = syllable.get('startTime', 0.0)

                    onsets.append(SongMapOnset(
                        time=onset_time,
                        section_index=section_idx,
                        line_index=line_idx,
                        syllable_index=syllable_idx,
                        chord=syllable.get('chord'),
                        confidence=1.0  # From offline analysis
                    ))

        # Sort by time
        onsets.sort(key=lambda o: o.time)

        return onsets

    def start(self) -> None:
        """Start tracking performance."""
        self.performance_start_time = time.time()
        self.last_update_time = self.performance_start_time
        self.position = PerformerPosition(
            song_time=0.0,
            section_index=0,
            line_index=0,
            syllable_index=0,
            confidence=0.0,
            tempo_ratio=1.0
        )
        self.recent_onsets.clear()
        self.tempo_estimates.clear()
        self.expected_next_onset_idx = 0

    def update(self, onset_detected: bool, current_time: Optional[float] = None) -> PerformerPosition:
        """
        Update position based on onset detection.

        Args:
            onset_detected: Whether an onset was detected in this audio block
            current_time: Optional explicit timestamp (otherwise uses time.time())

        Returns:
            Current performer position
        """
        if current_time is None:
            current_time = time.time()

        if self.performance_start_time is None:
            self.start()

        # Calculate elapsed time since performance started
        performance_elapsed = current_time - self.performance_start_time

        if onset_detected:
            self._handle_onset(performance_elapsed)
        else:
            # No onset - extrapolate position based on tempo
            self._extrapolate_position(performance_elapsed)

        self.last_update_time = current_time
        return self.position

    def _handle_onset(self, performance_time: float) -> None:
        """Handle a detected onset."""
        self.recent_onsets.append(performance_time)

        # Find matching onset in Song Map
        match_idx, confidence = self._find_matching_onset(performance_time)

        if match_idx is not None and confidence >= self.min_onset_confidence:
            # Found a confident match
            matched_onset = self.onsets[match_idx]

            # Update position
            self.position.song_time = matched_onset.time
            self.position.section_index = matched_onset.section_index
            self.position.line_index = matched_onset.line_index
            self.position.syllable_index = matched_onset.syllable_index
            self.position.confidence = confidence
            self.position.last_onset_time = performance_time

            # Update tempo estimate
            self._update_tempo_estimate(match_idx, performance_time)

            # Update expected next onset
            self.expected_next_onset_idx = match_idx + 1

    def _find_matching_onset(self, performance_time: float) -> Tuple[Optional[int], float]:
        """
        Find the Song Map onset that best matches the detected onset.

        Returns:
            (onset_index, confidence) or (None, 0.0) if no good match
        """
        if not self.onsets:
            return None, 0.0

        # Estimate where we should be in the song based on last known position
        expected_song_time = self.position.song_time
        if self.position.last_onset_time is not None:
            time_since_last = performance_time - self.position.last_onset_time
            expected_song_time += time_since_last * self.position.tempo_ratio
        else:
            # First onset - use performance time as song time (assume starting from 0)
            expected_song_time = performance_time

        # Search for onsets near the expected position
        search_center_idx = bisect.bisect_left(self.onset_times, expected_song_time)

        # Check onsets in a window around expected position
        best_idx = None
        best_confidence = 0.0

        # Search forward and backward from expected position
        for offset in range(-10, 11):  # Check Â±10 onsets
            idx = search_center_idx + offset

            if idx < 0 or idx >= len(self.onsets):
                continue

            onset = self.onsets[idx]

            # Calculate how far this onset is from where we expect to be
            song_time_diff = abs(onset.time - expected_song_time)

            # If within match window, calculate confidence
            if song_time_diff <= self.onset_match_window:
                # Higher confidence for closer matches
                confidence = 1.0 - (song_time_diff / self.onset_match_window)

                # Boost confidence if this is the expected next onset
                if idx == self.expected_next_onset_idx:
                    confidence = min(1.0, confidence * 1.5)

                if confidence > best_confidence:
                    best_confidence = confidence
                    best_idx = idx

        return best_idx, best_confidence

    def _update_tempo_estimate(self, onset_idx: int, performance_time: float) -> None:
        """Update tempo ratio estimate based on matched onset."""
        if self.position.last_onset_time is None:
            return

        # Calculate actual time between onsets
        performance_interval = performance_time - self.position.last_onset_time

        # Calculate expected interval from Song Map
        if onset_idx > 0:
            map_interval = self.onsets[onset_idx].time - self.onsets[onset_idx - 1].time

            if map_interval > 0 and performance_interval > 0:
                # Tempo ratio = map_time / performance_time
                # If performer is slower (takes longer), ratio < 1.0
                # If performer is faster, ratio > 1.0
                tempo_ratio = map_interval / performance_interval

                # Clamp to reasonable range (0.5x to 2x speed)
                tempo_ratio = np.clip(tempo_ratio, 0.5, 2.0)

                self.tempo_estimates.append(tempo_ratio)

                # Smooth tempo estimate
                if self.tempo_estimates:
                    median_tempo = np.median(list(self.tempo_estimates))
                    self.position.tempo_ratio = (
                        self.tempo_smoothing * median_tempo +
                        (1 - self.tempo_smoothing) * self.position.tempo_ratio
                    )

    def _extrapolate_position(self, performance_time: float) -> None:
        """Extrapolate position when no onset is detected."""
        if self.position.last_onset_time is None:
            # No onset detected yet, stay at start
            self.position.confidence = 0.0
            return

        # Calculate how much time has passed
        time_since_last = performance_time - self.position.last_onset_time

        # Extrapolate song position using tempo ratio
        song_time_advance = time_since_last * self.position.tempo_ratio
        self.position.song_time += song_time_advance

        # Decrease confidence over time when extrapolating
        # Confidence decays to 0.5 after 1 second without an onset
        decay_factor = np.exp(-time_since_last * 0.693)  # Half-life of 1 second
        self.position.confidence *= decay_factor

        # Update section/line/syllable based on new song_time
        self._update_position_from_song_time()

    def _update_position_from_song_time(self) -> None:
        """Update section/line/syllable indices based on current song_time."""
        # Find current onset based on song time
        idx = bisect.bisect_left(self.onset_times, self.position.song_time)

        if idx > 0 and idx <= len(self.onsets):
            onset = self.onsets[idx - 1]
            self.position.section_index = onset.section_index
            self.position.line_index = onset.line_index
            self.position.syllable_index = onset.syllable_index

    def get_current_section(self) -> Optional[SongMapSection]:
        """Get current section."""
        if 0 <= self.position.section_index < len(self.sections):
            return self.sections[self.position.section_index]
        return None

    def get_current_line(self) -> Optional[Dict[str, Any]]:
        """Get current line."""
        section = self.get_current_section()
        if section and 0 <= self.position.line_index < len(section.lines):
            return section.lines[self.position.line_index]
        return None

    def get_current_syllable(self) -> Optional[Dict[str, Any]]:
        """Get current syllable."""
        line = self.get_current_line()
        if line:
            syllables = line.get('syllables', [])
            if 0 <= self.position.syllable_index < len(syllables):
                return syllables[self.position.syllable_index]
        return None

    def get_lookahead(self, seconds: float = 2.0) -> List[Dict[str, Any]]:
        """
        Get upcoming syllables for teleprompter lookahead.

        Args:
            seconds: How many seconds ahead to look

        Returns:
            List of upcoming syllable dictionaries
        """
        lookahead_until = self.position.song_time + seconds
        upcoming = []

        for onset in self.onsets:
            if onset.time > self.position.song_time and onset.time <= lookahead_until:
                syllable = self._get_syllable_at_position(
                    onset.section_index,
                    onset.line_index,
                    onset.syllable_index
                )
                if syllable:
                    upcoming.append(syllable)

        return upcoming

    def _get_syllable_at_position(
        self,
        section_idx: int,
        line_idx: int,
        syllable_idx: int
    ) -> Optional[Dict[str, Any]]:
        """Get syllable at specific position."""
        if 0 <= section_idx < len(self.sections):
            section = self.sections[section_idx]
            if 0 <= line_idx < len(section.lines):
                line = section.lines[line_idx]
                syllables = line.get('syllables', [])
                if 0 <= syllable_idx < len(syllables):
                    return syllables[syllable_idx]
        return None

    def get_stats(self) -> Dict[str, Any]:
        """Get current tracking statistics."""
        return {
            'song_time': self.position.song_time,
            'section': self.get_current_section().name if self.get_current_section() else None,
            'confidence': self.position.confidence,
            'tempo_ratio': self.position.tempo_ratio,
            'tempo_bpm_ratio': self.position.tempo_ratio,  # Same as tempo_ratio
            'recent_onsets': len(self.recent_onsets),
            'total_sections': len(self.sections),
            'total_onsets': len(self.onsets)
        }
